{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 2: Linear models\n",
    "**Basics of modeling, optimization, and regularization**\n",
    "\n",
    "Joaquin Vanschoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Auto-setup when running on Google Colab\n",
    "import os\n",
    "if 'google.colab' in str(get_ipython()) and not os.path.exists('/content/master'):\n",
    "    !git clone -q https://github.com/ML-course/master.git /content/master\n",
    "    !pip --quiet install -r /content/master/requirements_colab.txt\n",
    "    %cd master/notebooks\n",
    "\n",
    "# Global imports and settings\n",
    "%matplotlib inline\n",
    "from preamble import *\n",
    "interactive = True # Set to True for interactive plots\n",
    "if interactive:\n",
    "    fig_scale = 0.5\n",
    "    plt.rcParams.update(print_config)\n",
    "else: # For printing\n",
    "    fig_scale = 0.3\n",
    "    plt.rcParams.update(print_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notation and Definitions \n",
    "* A _scalar_ is a simple numeric value, denoted by an italic letter: $x=3.24$\n",
    "* A _vector_ is a 1D ordered array of _n_ scalars, denoted by a bold letter: $\\mathbf{x}=[3.24, 1.2]$\n",
    "    * $x_i$ denotes the $i$th element of a vector, thus $x_0 = 3.24$.\n",
    "        * Note: some other courses use $x^{(i)}$ notation\n",
    "* A _set_ is an _unordered_ collection of unique elements, denote by caligraphic capital: $\\mathcal{S}=\\{3.24, 1.2\\}$\n",
    "* A _matrix_ is a 2D array of scalars, denoted by bold capital: $\\mathbf{X}=\\begin{bmatrix}\n",
    "3.24 & 1.2 \\\\\n",
    "2.24 & 0.2 \n",
    "\\end{bmatrix}$\n",
    "    * $\\textbf{X}_{i}$ denotes the $i$th _row_ of the matrix\n",
    "    * $\\textbf{X}_{:,j}$ denotes the $j$th _column_\n",
    "    * $\\textbf{X}_{i,j}$ denotes the _element_ in the $i$th row, $j$th column, thus $\\mathbf{X}_{1,0} = 2.24$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $\\mathbf{X}^{n \\times p}$, an $n \\times p$ matrix, can represent $n$ data points in a $p$-dimensional space \n",
    "    * Every row is a vector that can represent a _point_ in an p-dimensional space, given a _basis_.\n",
    "    * The _standard basis_ for a Euclidean space is the set of unit vectors\n",
    "* E.g. if $\\mathbf{X}=\\begin{bmatrix}\n",
    "3.24 & 1.2 \\\\\n",
    "2.24 & 0.2 \\\\\n",
    "3.0 & 0.6 \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "X = np.array([[3.24 , 1.2 ],[2.24, 0.2],[3.0 , 0.6 ]]) \n",
    "fig = plt.figure(figsize=(5*fig_scale,4*fig_scale))\n",
    "plt.scatter(X[:,0],X[:,1]);\n",
    "for i in range(3):\n",
    "    plt.annotate(i, (X[i,0]+0.02, X[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* A _tensor_ is an _k_-dimensional array of data, denoted by an italic capital: $T$\n",
    "    * _k_ is also called the order, degree, or rank\n",
    "    * $T_{i,j,k,...}$ denotes the element or sub-tensor in the corresponding position\n",
    "    * A set of color images can be represented by:\n",
    "        * a 4D tensor (sample x height x width x color channel)\n",
    "        * a 2D tensor (sample x flattened vector of pixel values)\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/08_images.png\" alt=\"ml\" style=\"width: 40%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic operations\n",
    "* Sums and products are denoted by capital Sigma and capital Pi:\n",
    "\n",
    "$$\\sum_{i=0}^{p} = x_0 + x_1 + ... + x_p \\quad \\prod_{i=0}^{p} = x_0 \\cdot x_1 \\cdot ... \\cdot x_p$$\n",
    "\n",
    "* Operations on vectors are element-wise: e.g. $\\mathbf{x}+\\mathbf{z} = [x_0+z_0,x_1+z_1, ... , x_p+z_p]$\n",
    "* Dot product $\\mathbf{w}\\mathbf{x} = \\mathbf{w} \\cdot \\mathbf{x} = \\mathbf{w}^{T} \\mathbf{x} = \\sum_{i=0}^{p} w_i \\cdot x_i = w_0 \\cdot x_0 + w_1 \\cdot x_1 + ... + w_p \\cdot x_p$\n",
    "* Matrix product $\\mathbf{W}\\mathbf{x} = \\begin{bmatrix}\n",
    "\\mathbf{w_0} \\cdot \\mathbf{x} \\\\\n",
    "... \\\\\n",
    "\\mathbf{w_p} \\cdot \\mathbf{x} \\end{bmatrix}$\n",
    "* A function $f(x) = y$ relates an input element $x$ to an output $y$\n",
    "    * It has a _local minimum_ at $x=c$ if $f(x) \\geq f(c)$ in interval $(c-\\epsilon, c+\\epsilon)$\n",
    "    * It has a _global minimum_ at $x=c$ if $f(x) \\geq f(c)$ for any value for $x$\n",
    "* A vector function consumes an input and produces a vector: $\\mathbf{f}(\\mathbf{x}) = \\mathbf{y}$\n",
    "* $\\underset{x\\in X}{\\operatorname{max}}f(x)$ returns the largest value f(x) for any x\n",
    "* $\\underset{x\\in X}{\\operatorname{argmax}}f(x)$ returns the element x that maximizes f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradients\n",
    "* A _derivative_ $f'$ of a function $f$ describes how fast $f$ grows or decreases\n",
    "* The process of finding a derivative is called differentiation\n",
    "    * Derivatives for basic functions are known\n",
    "    * For non-basic functions we use the chain rule: $F(x) = f(g(x)) \\rightarrow F'(x)=f'(g(x))g'(x)$\n",
    "* A function is _differentiable_ if it has a derivative in any point of it's domain\n",
    "    * It's _continuously differentiable_ if $f'$ is a continuous function\n",
    "    * We say $f$ is _smooth_ if it is _infinitely differentiable_, i.e., $f', f'', f''', ...$ all exist\n",
    "* A _gradient_ $\\nabla f$ is the derivative of a function in multiple dimensions\n",
    "    * It is a vector of partial derivatives: $\\nabla f = \\left[ \\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1},... \\right]$\n",
    "    * E.g. $f=2x_0+3x_1^{2}-\\sin(x_2) \\rightarrow \\nabla f= [2, 6x_1, -cos(x_2)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Example: $f = -(x_0^2+x_1^2)$\n",
    "    * $\\nabla f = \\left[\\frac{\\partial f}{\\partial x_0},\\frac{\\partial f}{\\partial x_1}\\right] = \\left[-2x_0,-2x_1\\right]$\n",
    "    * Evaluated at point (-4,1): $\\nabla f(-4,1) = [8,-2]$\n",
    "        * These are the slopes at point (-4,1) in the direction of $x_0$ and $x_1$ respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "# f = -(x0^2 + x1^2)\n",
    "def g_f(x0, x1):\n",
    "    return -(x0 ** 2 + x1 ** 2)\n",
    "def g_dfx0(x0):\n",
    "    return -2 * x0\n",
    "def g_dfx1(x1):\n",
    "    return -2 * x1\n",
    "\n",
    "@interact\n",
    "def plot_gradient(rotation=(0,240,10)):\n",
    "    # plot surface of f\n",
    "    fig = plt.figure(figsize=(12*fig_scale,5*fig_scale))\n",
    "    ax = plt.axes(projection=\"3d\")\n",
    "    x0 = np.linspace(-6, 6, 30)\n",
    "    x1 = np.linspace(-6, 6, 30)\n",
    "    X0, X1 = np.meshgrid(x0, x1)\n",
    "    ax.plot_surface(X0, X1, g_f(X0, X1), rstride=1, cstride=1,\n",
    "                    cmap='winter', edgecolor='none',alpha=0.3)\n",
    "\n",
    "    # choose point to evaluate: (-4,1)\n",
    "    i0 = -4\n",
    "    i1 = 1\n",
    "    iz = np.linspace(g_f(i0,i1), -82, 30)\n",
    "    ax.scatter3D(i0, i1, g_f(i0,i1), c=\"k\", s=20*fig_scale,label='($i_0$,$i_1$) = (-4,1)')\n",
    "    ax.plot3D([i0]*30, [i1]*30, iz, linewidth=1*fig_scale, c='silver', linestyle='-')\n",
    "    ax.set_zlim(-80,0)\n",
    "\n",
    "    # plot intersects\n",
    "    ax.plot3D(x0,[1]*30,g_f(x0, 1),linewidth=3*fig_scale,alpha=0.9,label='$f(x_0,i_1)$',c='r',linestyle=':')\n",
    "    ax.plot3D([-4]*30,x1,g_f(-4, x1),linewidth=3*fig_scale,alpha=0.9,label='$f(i_0,x_1)$',c='b',linestyle=':')\n",
    "\n",
    "    # df/dx0 is slope of line at the intersect point\n",
    "    x0 = np.linspace(-8, 0, 30)\n",
    "    ax.plot3D(x0,[1]*30,g_dfx0(i0)*x0-g_f(i0,i1),linewidth=3*fig_scale,label=r'$\\frac{\\partial f}{\\partial x_0}(i_0,i_1) x_0 + f(i_0,i_1)$',c='r',linestyle='-')\n",
    "    ax.plot3D([-4]*30,x1,g_dfx1(i1)*x1+g_f(i0,i1),linewidth=3*fig_scale,label=r'$\\frac{\\partial f}{\\partial x_1}(i_0,i_1) x_1 + f(i_0,i_1)$',c='b',linestyle='-')\n",
    "\n",
    "    ax.set_xlabel('x0', labelpad=-4/fig_scale)\n",
    "    ax.set_ylabel('x1', labelpad=-4/fig_scale)\n",
    "    ax.get_zaxis().set_ticks([])\n",
    "    ax.view_init(30, rotation) # Use this to rotate the figure\n",
    "    ax.legend()\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.tick_params(axis='both', width=0, labelsize=10*fig_scale, pad=-6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_gradient(rotation=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distributions and Probabilities\n",
    "* The normal (Gaussian) distribution with mean $\\mu$ and standard deviation $\\sigma$ is noted as $N(\\mu,\\sigma)$\n",
    "* A random variable $X$ can be continuous or discrete\n",
    "* A probability distribution $f_X$ of a continuous variable $X$: _probability density function_ (pdf)\n",
    "    * The _expectation_ is given by $\\mathbb{E}[X] = \\int x f_{X}(x) dx$\n",
    "* A probability distribution of a discrete variable: _probability mass function_ (pmf)\n",
    "    * The _expectation_ (or mean) $\\mu_X = \\mathbb{E}[X] = \\sum_{i=1}^k[x_i \\cdot Pr(X=x_i)]$\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/02_pdf.png\" alt=\"ml\" style=\"width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear models\n",
    "Linear models make a prediction using a linear function of the input features $X$ \n",
    "\n",
    "$$f_{\\mathbf{w}}(\\mathbf{x}) = \\sum_{i=1}^{p} w_i \\cdot x_i + w_{0}$$\n",
    "\n",
    "Learn $w$ from $X$, given a loss function $\\mathcal{L}$:\n",
    "\n",
    "$$\\underset{\\mathbf{w}}{\\operatorname{argmin}} \\mathcal{L}(f_\\mathbf{w}(X))$$\n",
    "\n",
    "* Many algorithms with different $\\mathcal{L}$: Least squares, Ridge, Lasso, Logistic Regression, Linear SVMs,...\n",
    "* Can be very powerful (and fast), especially for large datasets with many features.\n",
    "* Can be generalized to learn non-linear patterns: _Generalized Linear Models_\n",
    "    * Features can be augmentented with polynomials of the original features\n",
    "    * Features can be transformed according to a distribution (Poisson, Tweedie, Gamma,...)\n",
    "    * Some linear models (e.g. SVMs) can be _kernelized_ to learn non-linear functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear models for regression\n",
    "* Prediction formula for input features x:\n",
    "    * $w_1$ ... $w_p$ usually called _weights_ or _coefficients_ , $w_0$ the _bias_ or _intercept_\n",
    "    * Assumes that errors are $N(0,\\sigma)$\n",
    "\n",
    "$$\\hat{y} = \\mathbf{w}\\mathbf{x} + w_0 = \\sum_{i=1}^{p} w_i \\cdot x_i + w_0 = w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_p \\cdot x_p + w_0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mglearn.datasets import make_wave\n",
    "\n",
    "Xw, yw = make_wave(n_samples=60)\n",
    "Xw_train, Xw_test, yw_train, yw_test = train_test_split(Xw, yw, random_state=42)\n",
    "\n",
    "line = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "\n",
    "lr = LinearRegression().fit(Xw_train, yw_train)\n",
    "print(\"w_1: %f  w_0: %f\" % (lr.coef_[0], lr.intercept_))\n",
    "\n",
    "plt.figure(figsize=(6*fig_scale, 3*fig_scale))\n",
    "plt.plot(line, lr.predict(line), lw=fig_scale)\n",
    "plt.plot(Xw_train, yw_train, 'o', c='b')\n",
    "#plt.plot(X_test, y_test, '.', c='r')\n",
    "ax = plt.gca()\n",
    "ax.grid(True)\n",
    "ax.set_ylim(-2, 2)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.legend([\"model\", \"training data\"], loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression (aka Ordinary Least Squares)\n",
    "* Loss function is the _sum of squared errors_ (SSE) (or residuals) between predictions $\\hat{y}_i$ (red) and the true regression targets $y_i$ (blue) on the training set.\n",
    "\n",
    "$$\\mathcal{L}_{SSE} = \\sum_{n=1}^{N} (y_n-\\hat{y}_n)^2 = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2$$ \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/02_least_squares.png\" alt=\"ml\" style=\"margin: 0 auto; width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solving ordinary least squares\n",
    "* Convex optimization problem with unique closed-form solution:\n",
    "    \n",
    "    $$w^{*} = (X^{T}X)^{-1} X^T Y$$\n",
    "    \n",
    "    * Add a column of 1's to the front of X to get $w_0$\n",
    "    * Slow. Time complexity is quadratic in number of features: $\\mathcal{O}(p^2n)$\n",
    "        * X has $n$ rows, $p$ features, hence $X^{T}X$ has dimensionality $p \\cdot p$\n",
    "    * Only works if $n>p$\n",
    "* _Gradient Descent_\n",
    "    * Faster for large and/or high-dimensional datasets\n",
    "    * When $X^{T}X$ cannot be computed or takes too long ($p$ or $n$ is too large)\n",
    "    * When you want more control over the learning process\n",
    "* **Very easily overfits**.\n",
    "    * coefficients $w$ become very large (steep incline/decline)\n",
    "    * small change in the input *x* results in a very different output *y* \n",
    "    * No hyperparameters that control model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Gradient Descent\n",
    "* Start with an initial, random set of weights: $\\mathbf{w}^0$\n",
    "* Given a differentiable loss function $\\mathcal{L}$ (e.g. $\\mathcal{L}_{SSE}$), compute $\\nabla \\mathcal{L}$\n",
    "* For least squares: $\\frac{\\partial \\mathcal{L}_{SSE}}{\\partial w_i}(\\mathbf{w}) = -2 \\sum_{n=1}^{N} (y_n-\\hat{y}_n) x_{n,i}$\n",
    "    * If feature $X_{:,i}$ is associated with big errors, the gradient wrt $w_i$ will be large\n",
    "* Update _all_ weights slightly (by _step size_ or _learning rate_ $\\eta$) in 'downhill' direction.\n",
    "* Basic _update rule_ (step s): \n",
    "\n",
    "    $$\\mathbf{w}^{s+1} = \\mathbf{w}^s-\\eta\\nabla \\mathcal{L}(\\mathbf{w}^s)$$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_gradient_descent.jpg\" alt=\"ml\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Important hyperparameters\n",
    "    * Learning rate\n",
    "        * Too small: slow convergence. Too large: possible divergence\n",
    "    * Maximum number of iterations\n",
    "        * Too small: no convergence. Too large: wastes resources\n",
    "    * Learning rate decay with decay rate $k$\n",
    "        * E.g. exponential ($\\eta^{s+1} = \\eta^{0}  e^{-ks}$), inverse-time ($\\eta^{s+1} = \\frac{\\eta^{s}}{1+ks}$),...\n",
    "    * Many more advanced ways to control learning rate (see later)\n",
    "        * Adaptive techniques: depend on how much loss improved in previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# Some convex function to represent the loss\n",
    "def l_fx(x): \n",
    "    return (x * 4)**2 \n",
    "# Derivative to compute the gradient\n",
    "def l_dfx0(x0):\n",
    "    return 8 * x0\n",
    "\n",
    "@interact\n",
    "def plot_learning_rate(learn_rate=(0.01,0.4,0.01), exp_decay=False):\n",
    "    w = np.linspace(-1,1,101)\n",
    "    f = [l_fx(i) for i in w]\n",
    "    w_current = -0.75\n",
    "    learn_rate_current = learn_rate\n",
    "    fw = [] # weight values\n",
    "    fl = [] # loss values\n",
    "    for i in range(10):\n",
    "        fw.append(w_current)\n",
    "        fl.append(l_fx(w_current))\n",
    "        # Decay\n",
    "        if exp_decay:\n",
    "            learn_rate_current = learn_rate * math.exp(-0.3*i)\n",
    "        # Update rule\n",
    "        w_current = w_current - learn_rate_current * l_dfx0(w_current)\n",
    "    fig, ax = plt.subplots(figsize=(5*fig_scale,3*fig_scale))\n",
    "    ax.set_xlabel('w')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.plot(w, f, lw=2*fig_scale, ls='-', c='k', label='Loss')\n",
    "    ax.plot(fw, fl, '--bo', lw=2*fig_scale, markersize=3)\n",
    "    plt.ylim(-1,16)\n",
    "    plt.xlim(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_learning_rate(learn_rate=0.21, exp_decay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Toy surface\n",
    "def f(x, y):\n",
    "    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n",
    "\n",
    "# Tensorflow optimizers\n",
    "sgd = tf.optimizers.SGD(0.01)\n",
    "lr_schedule = tf.optimizers.schedules.ExponentialDecay(0.02,decay_steps=100,decay_rate=0.96)\n",
    "sgd_decay = tf.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "optimizers = [sgd, sgd_decay]\n",
    "opt_names = ['sgd', 'sgd_decay']\n",
    "cmap = plt.cm.get_cmap('tab10')\n",
    "colors = [cmap(x/10) for x in range(10)]\n",
    "\n",
    "# Training\n",
    "all_paths = []\n",
    "for opt, name in zip(optimizers, opt_names):\n",
    "    x = tf.Variable(0.8)\n",
    "    y = tf.Variable(1.6)\n",
    "\n",
    "    x_history = []\n",
    "    y_history = []\n",
    "    loss_prev = 0.0\n",
    "    max_steps = 100\n",
    "    for step in range(max_steps):\n",
    "        with tf.GradientTape() as g:\n",
    "            loss = f(x, y)\n",
    "            x_history.append(x.numpy())\n",
    "            y_history.append(y.numpy())\n",
    "            grads = g.gradient(loss, [x, y])\n",
    "            opt.apply_gradients(zip(grads, [x, y]))\n",
    "\n",
    "    if np.abs(loss_prev - loss.numpy()) < 1e-6:\n",
    "        break\n",
    "    loss_prev = loss.numpy()\n",
    "    x_history = np.array(x_history)\n",
    "    y_history = np.array(y_history)\n",
    "    path = np.concatenate((np.expand_dims(x_history, 1), np.expand_dims(y_history, 1)), axis=1).T\n",
    "    all_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "import tensorflow as tf\n",
    "\n",
    "# Toy surface\n",
    "def f(x, y):\n",
    "    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n",
    "\n",
    "# Tensorflow optimizers\n",
    "sgd = tf.optimizers.SGD(0.01)\n",
    "lr_schedule = tf.optimizers.schedules.ExponentialDecay(0.02,decay_steps=100,decay_rate=0.96)\n",
    "sgd_decay = tf.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "optimizers = [sgd, sgd_decay]\n",
    "opt_names = ['sgd', 'sgd_decay']\n",
    "cmap = plt.cm.get_cmap('tab10')\n",
    "colors = [cmap(x/10) for x in range(10)]\n",
    "\n",
    "# Training\n",
    "all_paths = []\n",
    "for opt, name in zip(optimizers, opt_names):\n",
    "    x_init = 0.8\n",
    "    x = tf.Variable(x_init)\n",
    "    y_init = 1.6\n",
    "    y = tf.Variable(y_init)\n",
    "\n",
    "    x_history = []\n",
    "    y_history = []\n",
    "    z_prev = 0.0\n",
    "    max_steps = 100\n",
    "    for step in range(max_steps):\n",
    "        with tf.GradientTape() as g:\n",
    "            z = f(x, y)\n",
    "            x_history.append(x.numpy())\n",
    "            y_history.append(y.numpy())\n",
    "            dz_dx, dz_dy = g.gradient(z, [x, y])\n",
    "            opt.apply_gradients(zip([dz_dx, dz_dy], [x, y]))\n",
    "\n",
    "    if np.abs(z_prev - z.numpy()) < 1e-6:\n",
    "        break\n",
    "    z_prev = z.numpy()\n",
    "    x_history = np.array(x_history)\n",
    "    y_history = np.array(y_history)\n",
    "    path = np.concatenate((np.expand_dims(x_history, 1), np.expand_dims(y_history, 1)), axis=1).T\n",
    "    all_paths.append(path)\n",
    "        \n",
    "# Plotting\n",
    "number_of_points = 50\n",
    "margin = 4.5\n",
    "minima = np.array([3., .5])\n",
    "minima_ = minima.reshape(-1, 1)\n",
    "x_min = 0. - 2\n",
    "x_max = 0. + 3.5\n",
    "y_min = 0. - 3.5\n",
    "y_max = 0. + 2\n",
    "x_points = np.linspace(x_min, x_max, number_of_points) \n",
    "y_points = np.linspace(y_min, y_max, number_of_points)\n",
    "x_mesh, y_mesh = np.meshgrid(x_points, y_points)\n",
    "z = np.array([f(xps, yps) for xps, yps in zip(x_mesh, y_mesh)])\n",
    "\n",
    "def plot_optimizers(ax, iterations, optimizers):\n",
    "    ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-0.5, 5, 25), norm=LogNorm(), cmap=plt.cm.jet, linewidths=fig_scale, zorder=-1)\n",
    "    ax.plot(*minima, 'r*', markersize=20*fig_scale)\n",
    "    for name, path, color in zip(opt_names, all_paths, colors):\n",
    "        if name in optimizers:\n",
    "            p = path[:,:iterations]\n",
    "            ax.plot([], [], color=color, label=name, lw=3*fig_scale, linestyle='-')\n",
    "            ax.quiver(p[0,:-1], p[1,:-1], p[0,1:]-p[0,:-1], p[1,1:]-p[1,:-1], scale_units='xy', angles='xy', scale=1, color=color, lw=4)\n",
    "\n",
    "\n",
    "    ax.set_xlim((x_min, x_max))\n",
    "    ax.set_ylim((y_min, y_max))\n",
    "    ax.legend(loc='lower left', prop={'size': 15*fig_scale}) \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from decimal import *\n",
    "\n",
    "# Training for momentum\n",
    "all_lr_paths = []\n",
    "lr_range = [0.005 * i for i in range(0,10)]\n",
    "for lr in lr_range:\n",
    "    opt = tf.optimizers.SGD(lr, nesterov=False)\n",
    "\n",
    "    x_init = 0.8\n",
    "    x = tf.compat.v1.get_variable('x', dtype=tf.float32, initializer=tf.constant(x_init))\n",
    "    y_init = 1.6\n",
    "    y = tf.compat.v1.get_variable('y', dtype=tf.float32, initializer=tf.constant(y_init))\n",
    "\n",
    "    x_history = []\n",
    "    y_history = []\n",
    "    z_prev = 0.0\n",
    "    max_steps = 100\n",
    "    for step in range(max_steps):\n",
    "        with tf.GradientTape() as g:\n",
    "            z = f(x, y)\n",
    "            x_history.append(x.numpy())\n",
    "            y_history.append(y.numpy())\n",
    "            dz_dx, dz_dy = g.gradient(z, [x, y])\n",
    "            opt.apply_gradients(zip([dz_dx, dz_dy], [x, y]))\n",
    "\n",
    "    if np.abs(z_prev - z.numpy()) < 1e-6:\n",
    "        break\n",
    "    z_prev = z.numpy()\n",
    "    x_history = np.array(x_history)\n",
    "    y_history = np.array(y_history)\n",
    "    path = np.concatenate((np.expand_dims(x_history, 1), np.expand_dims(y_history, 1)), axis=1).T\n",
    "    all_lr_paths.append(path)\n",
    "    \n",
    "# Plotting\n",
    "number_of_points = 50\n",
    "margin = 4.5\n",
    "minima = np.array([3., .5])\n",
    "minima_ = minima.reshape(-1, 1)\n",
    "x_min = 0. - 2\n",
    "x_max = 0. + 3.5\n",
    "y_min = 0. - 3.5\n",
    "y_max = 0. + 2\n",
    "x_points = np.linspace(x_min, x_max, number_of_points) \n",
    "y_points = np.linspace(y_min, y_max, number_of_points)\n",
    "x_mesh, y_mesh = np.meshgrid(x_points, y_points)\n",
    "z = np.array([f(xps, yps) for xps, yps in zip(x_mesh, y_mesh)])\n",
    "\n",
    "def plot_learning_rate_optimizers(ax, iterations, lr):\n",
    "    ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-0.5, 5, 25), norm=LogNorm(), cmap=plt.cm.jet, linewidths=fig_scale, zorder=-1)\n",
    "    ax.plot(*minima, 'r*', markersize=20*fig_scale)\n",
    "    for path, lrate in zip(all_lr_paths, lr_range):\n",
    "        if round(lrate,3) == lr:\n",
    "            p = path[:,:iterations]\n",
    "            ax.plot([], [], color='b', label=\"Learning rate {}\".format(lr), lw=3*fig_scale, linestyle='-')\n",
    "            ax.quiver(p[0,:-1], p[1,:-1], p[0,1:]-p[0,:-1], p[1,1:]-p[1,:-1], scale_units='xy', angles='xy', scale=1, color='b', lw=4)\n",
    "\n",
    "\n",
    "    ax.set_xlim((x_min, x_max))\n",
    "    ax.set_ylim((y_min, y_max))\n",
    "    ax.legend(loc='lower left', prop={'size': 15*fig_scale}) \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Effect of learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def plot_lr(iterations=(1,100,1), learning_rate=(0.005,0.04,0.005)):\n",
    "    fig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\n",
    "    plot_learning_rate_optimizers(ax,iterations,learning_rate)\n",
    "    \n",
    "if not interactive:\n",
    "    plot_lr(iterations=50, learning_rate=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Effect of learning rate decay**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def compare_optimizers(iterations=(1,100,1), optimizer1=opt_names, optimizer2=opt_names):\n",
    "    fig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\n",
    "    plot_optimizers(ax,iterations,[optimizer1,optimizer2])\n",
    "    \n",
    "if not interactive:\n",
    "    compare_optimizers(iterations=50, optimizer1=\"sgd\", optimizer2=\"sgd_decay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In two dimensions:\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_gradient_descent_2D.png\" alt=\"ml\" style=\"width: 900px;\"/>\n",
    "\n",
    "* You can get stuck in local minima (if the loss is not fully convex)\n",
    "    * If you have many model parameters, this is less likely\n",
    "    * You always find a way down in some direction\n",
    "    * Models with many parameters typically find good local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Intuition: walking downhill using only the slope you \"feel\" nearby\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/01_gradient_descent_hill.png\" alt=\"ml\" style=\"width: 1200px;\"/>\n",
    "\n",
    "(Image by A. Karpathy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stochastic Gradient Descent (SGD)\n",
    "* Compute gradients not on the entire dataset, but on a single data point $i$ at a time\n",
    "    * Gradient descent: $\\mathbf{w}^{s+1} = \\mathbf{w}^s-\\eta\\nabla \\mathcal{L}(\\mathbf{w}^s) = \\mathbf{w}^s-\\frac{\\eta}{n} \\sum_{i=1}^{n} \\nabla \\mathcal{L_i}(\\mathbf{w}^s)$\n",
    "    * Stochastic Gradient Descent: $\\mathbf{w}^{s+1} = \\mathbf{w}^s-\\eta\\nabla \\mathcal{L_i}(\\mathbf{w}^s)$\n",
    "* Many smoother variants, e.g.\n",
    "    * Minibatch SGD: compute gradient on batches of data: $\\mathbf{w}^{s+1} = \\mathbf{w}^s-\\frac{\\eta}{B} \\sum_{i=1}^{B} \\nabla \\mathcal{L_i}(\\mathbf{w}^s)$\n",
    "    * Stochastic Average Gradient Descent ([SAG](https://link.springer.com/content/pdf/10.1007/s10107-016-1030-6.pdf), [SAGA](https://proceedings.neurips.cc/paper/2014/file/ede7e2b6d13a41ddf9f4bdef84fdc737-Paper.pdf)). With $i_s \\in [1,n]$ randomly chosen per iteration:\n",
    "        * Incremental gradient: $\\mathbf{w}^{s+1} = \\mathbf{w}^s-\\frac{\\eta}{n} \\sum_{i=1}^{n} v_i^s$ with $v_i^s = \\begin{cases}\\nabla \\mathcal{L_i}(\\mathbf{w}^s) & i = i_s \\\\ v_i^{s-1} & \\text{otherwise} \\end{cases}$\n",
    "        \n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/08_SGD.png\" alt=\"ml\" style=\"float: left; width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### In practice\n",
    "* Linear regression can be found in `sklearn.linear_model`. We'll evaluate it on the Boston Housing dataset.\n",
    "    * `LinearRegression` uses closed form solution, `SGDRegressor` with `loss='squared_loss'` uses Stochastic Gradient Descent\n",
    "    * Large coefficients signal overfitting\n",
    "    * Test score is much lower than training score\n",
    "\n",
    "``` python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_B, y_B = mglearn.datasets.load_extended_boston()\n",
    "X_B_train, X_B_test, y_B_train, y_B_test = train_test_split(X_B, y_B, random_state=0)\n",
    "\n",
    "lr = LinearRegression().fit(X_B_train, y_B_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Weights (coefficients): {}\".format(lr.coef_[0:40]))\n",
    "print(\"Bias (intercept): {}\".format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Training set score (R^2): {:.2f}\".format(lr.score(X_B_train, y_B_train)))\n",
    "print(\"Test set score (R^2): {:.2f}\".format(lr.score(X_B_test, y_B_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Ridge regression\n",
    "* Adds a penalty term to the least squares loss function:\n",
    "\n",
    "$$\\mathcal{L}_{Ridge} = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 + \\alpha \\sum_{i=1}^{p} w_i^2$$ \n",
    "\n",
    "* Model is penalized if it uses large coefficients ($w$)\n",
    "    * Each feature should have as little effect on the outcome as possible \n",
    "    * We don't want to penalize $w_0$, so we leave it out\n",
    "* Regularization: explicitly restrict a model to avoid overfitting. \n",
    "    * Called L2 regularization because it uses the L2 norm: $\\sum w_i^2$\n",
    "* The strength of the regularization can be controlled with the $\\alpha$ hyperparameter.\n",
    "    * Increasing $\\alpha$ causes more regularization (or shrinkage). Default is 1.0.\n",
    "* Still convex. Can be optimized in different ways:\n",
    "    * Closed form solution (a.k.a. Cholesky): $w^{*} = (X^{T}X + \\alpha I)^{-1} X^T Y$\n",
    "    * Gradient descent and variants, e.g. Stochastic Average Gradient (SAG,SAGA)\n",
    "        * Conjugate gradient (CG): each new gradient is influenced by previous ones\n",
    "    * Use Cholesky for smaller datasets, Gradient descent for larger ones\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### In practice\n",
    "``` python\n",
    "from sklearn.linear_model import Ridge\n",
    "lr = Ridge().fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge().fit(X_B_train, y_B_train)\n",
    "print(\"Weights (coefficients): {}\".format(ridge.coef_[0:40]))\n",
    "print(\"Bias (intercept): {}\".format(ridge.intercept_))\n",
    "print(\"Training set score: {:.2f}\".format(ridge.score(X_B_train, y_B_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge.score(X_B_test, y_B_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set score is higher and training set score lower: less overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* We can plot the weight values for differents levels of regularization to explore the effect of $\\alpha$.\n",
    "* Increasing regularization decreases the values of the coefficients, but never to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "@interact\n",
    "def plot_ridge(alpha=(0,10.0,0.05)):\n",
    "    r = Ridge(alpha=alpha).fit(X_B_train, y_B_train)\n",
    "    fig, ax = plt.subplots(figsize=(8*fig_scale,1.5*fig_scale))\n",
    "    ax.plot(r.coef_, 'o', markersize=3)\n",
    "    ax.set_title(\"alpha {}, score {:.2f} (training score {:.2f})\".format(alpha, r.score(X_B_test, y_B_test), r.score(X_B_train, y_B_train)))\n",
    "    ax.set_xlabel(\"Coefficient index\")\n",
    "    ax.set_ylabel(\"Coefficient magnitude\")\n",
    "    ax.hlines(0, 0, len(r.coef_))\n",
    "    ax.set_ylim(-25, 25)\n",
    "    ax.set_xlim(0, 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    for alpha in [0.1, 10]:\n",
    "        plot_ridge(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* When we plot the train and test scores for every $\\alpha$ value, we see a sweet spot around $\\alpha=0.2$\n",
    "    * Models with smaller $\\alpha$ are overfitting\n",
    "    * Models with larger $\\alpha$ are underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "alpha=np.logspace(-3,2,num=20)\n",
    "ai = list(range(len(alpha)))\n",
    "test_score=[]\n",
    "train_score=[]\n",
    "for a in alpha:\n",
    "    r = Ridge(alpha=a).fit(X_B_train, y_B_train)\n",
    "    test_score.append(r.score(X_B_test, y_B_test))\n",
    "    train_score.append(r.score(X_B_train, y_B_train))\n",
    "fig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\n",
    "ax.set_xticks(range(20))\n",
    "ax.set_xticklabels(np.round(alpha,3))\n",
    "ax.set_xlabel('alpha')\n",
    "ax.plot(test_score, lw=2*fig_scale, label='test score')\n",
    "ax.plot(train_score, lw=2*fig_scale, label='train score')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Other ways to reduce overfitting\n",
    "* Add more training data: with enough training data, regularization becomes less important\n",
    "    * Ridge and ordinary least squares will have the same performance\n",
    "* Use fewer features: remove unimportant ones or find a low-dimensional embedding (e.g. PCA)\n",
    "    * Fewer coefficients to learn, reduces the flexibility of the model\n",
    "* Scaling the data typically helps (and changes the optimal $\\alpha$ value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10*fig_scale,4*fig_scale))\n",
    "mglearn.plots.plot_ridge_n_samples(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lasso (Least Absolute Shrinkage and Selection Operator)\n",
    "* Adds a different penalty term to the least squares sum:\n",
    "\n",
    "$$\\mathcal{L}_{Lasso} = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 + \\alpha \\sum_{i=1}^{p} |w_i|$$\n",
    "\n",
    "* Called L1 regularization because it uses the L1 norm\n",
    "    * Will cause many weights to be exactly 0\n",
    "* Same parameter $\\alpha$ to control the strength of regularization. \n",
    "    * Will again have a 'sweet spot' depending on the data\n",
    "* No closed-form solution\n",
    "* Convex, but no longer strictly convex, and not differentiable\n",
    "    * Weights can be optimized using _coordinate descent_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Analyze what happens to the weights:\n",
    "* L1 prefers coefficients to be exactly zero (sparse models)\n",
    "* Some features are ignored entirely: automatic feature selection\n",
    "* How can we explain this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "@interact\n",
    "def plot_lasso(alpha=(0,0.5,0.005)):\n",
    "    r = Lasso(alpha=alpha).fit(X_B_train, y_B_train)\n",
    "    fig, ax = plt.subplots(figsize=(8*fig_scale,1.5*fig_scale))\n",
    "    ax.plot(r.coef_, 'o', markersize=6*fig_scale)\n",
    "    ax.set_title(\"alpha {}, score {:.2f} (training score {:.2f})\".format(alpha, r.score(X_B_test, y_B_test), r.score(X_B_train, y_B_train)), pad=0.5)\n",
    "    ax.set_xlabel(\"Coefficient index\", labelpad=0)\n",
    "    ax.set_ylabel(\"Coefficient magnitude\")\n",
    "    ax.hlines(0, 0, len(r.coef_))\n",
    "    ax.set_ylim(-25, 25);\n",
    "    ax.set_xlim(0, 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    for alpha in [0.00001, 0.01]:\n",
    "        plot_lasso(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Coordinate descent\n",
    "- Alternative for gradient descent, supports non-differentiable convex loss functions (e.g. $\\mathcal{L}_{Lasso}$)\n",
    "- In every iteration, optimize a single coordinate $w_i$ (find minimum in direction of $x_i$)\n",
    "    - Continue with another coordinate, using a selection rule (e.g. round robin)\n",
    "- Faster iterations. No need to choose a step size (learning rate).\n",
    "- May converge more slowly. Can't be parallellized.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/02_cd.png\" alt=\"ml\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Coordinate descent with Lasso\n",
    "\n",
    "- Remember that $\\mathcal{L}_{Lasso} = \\mathcal{L}_{SSE} + \\alpha \\sum_{i=1}^{p} |w_i|$ \n",
    "- For one $w_i$: $\\mathcal{L}_{Lasso}(w_i) = \\mathcal{L}_{SSE}(w_i) + \\alpha |w_i|$\n",
    "- The L1 term is not differentiable but convex: we can compute the [_subgradient_](https://towardsdatascience.com/unboxing-lasso-regularization-with-proximal-gradient-method-ista-iterative-soft-thresholding-b0797f05f8ea) \n",
    "    - Unique at points where $\\mathcal{L}$ is differentiable, a range of all possible slopes [a,b] where it is not\n",
    "    - For $|w_i|$, the subgradient $\\partial_{w_i} |w_i|$ =  $\\begin{cases}-1 & w_i<0\\\\ [-1,1] & w_i=0 \\\\ 1 & w_i>0 \\\\ \\end{cases}$\n",
    "\n",
    "    - Subdifferential $\\partial(f+g) = \\partial f + \\partial g$ if $f$ and $g$ are both convex\n",
    "- To find the optimum for Lasso $w_i^{*}$, solve\n",
    "\n",
    "    $$\\begin{aligned} \\partial_{w_i} \\mathcal{L}_{Lasso}(w_i) &= \\partial_{w_i} \\mathcal{L}_{SSE}(w_i) + \\partial_{w_i} \\alpha |w_i| \\\\ 0 &= (w_i - \\rho_i) + \\alpha \\cdot \\partial_{w_i} |w_i| \\\\ w_i &= \\rho_i - \\alpha \\cdot \\partial_{w_i} |w_i| \\end{aligned}$$\n",
    "\n",
    "    - In which $\\rho_i$ is the solution for $\\mathcal{L}_{SSE}(w_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We found: $w_i = \\rho_i - \\alpha \\cdot \\partial_{w_i} |w_i|$\n",
    "- Lasso solution has the form of a _soft thresholding function_ $S$\n",
    "\n",
    "    $$w_i^* = S(\\rho_i,\\alpha) = \\begin{cases} \\rho_i + \\alpha, & \\rho_i < -\\alpha \\\\  0, & -\\alpha < \\rho_i < \\alpha \\\\ \\rho_i - \\alpha, & \\rho_i > \\alpha \\\\ \\end{cases}$$\n",
    "    \n",
    "    - Small weights become 0: sparseness!\n",
    "    - If the data is not normalized, $w_i^* = \\frac{1}{z_i}S(\\rho_i,\\alpha)$ with $z_i$ a normalizing constant\n",
    "- Ridge solution: $w_i = \\rho_i - \\alpha \\cdot \\partial_{w_i} w_i^2 = \\rho_i - 2\\alpha \\cdot w_i$, thus $w_i^* = \\frac{\\rho_i}{1 + 2\\alpha}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def plot_rho(alpha=(0,2.0,0.05)):\n",
    "    w = np.linspace(-2,2,101)\n",
    "    r = w/(1+2*alpha)\n",
    "    l = [x+alpha if x <= -alpha else (x-alpha if x > alpha else 0) for x in w]\n",
    "    fig, ax = plt.subplots(figsize=(6*fig_scale,3*fig_scale))\n",
    "    ax.set_xlabel(r'$\\rho$')\n",
    "    ax.set_ylabel(r'$w^{*}$')\n",
    "    ax.plot(w, w, lw=2*fig_scale, c='g', label='Ordinary Least Squares (SSE)')\n",
    "    ax.plot(w, r, lw=2*fig_scale, c='b', label='Ridge with alpha={}'.format(alpha))\n",
    "    ax.plot(w, l, lw=2*fig_scale, c='r', label='Lasso with alpha={}'.format(alpha))\n",
    "    ax.legend()\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_rho(alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpreting L1 and L2 loss\n",
    "- L1 and L2 in function of the weights\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/L12_1.png\" alt=\"ml\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Least Squares Loss + L1 or L2\n",
    "- Lasso is not differentiable at point 0\n",
    "- For any minimum of least squares, L2 will be smaller, and L1 is more likely be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def c_fx(x):\n",
    "    fX = ((x * 2 - 1)**2) # Some convex function to represent the loss\n",
    "    return fX/9 # Scaling\n",
    "def c_fl2(x,alpha):\n",
    "    return c_fx(x) + alpha * x**2\n",
    "def c_fl1(x,alpha):\n",
    "    return c_fx(x) + alpha * abs(x)\n",
    "def l2(x,alpha):\n",
    "    return alpha * x**2\n",
    "def l1(x,alpha):\n",
    "    return alpha * abs(x)\n",
    "\n",
    "@interact\n",
    "def plot_losses(alpha=(0,1.0,0.05)):\n",
    "    w = np.linspace(-1,1,101)\n",
    "    f = [c_fx(i) for i in w]\n",
    "    r = [c_fl2(i,alpha) for i in w]\n",
    "    l = [c_fl1(i,alpha) for i in w]\n",
    "    rp = [l2(i,alpha) for i in w]\n",
    "    lp = [l1(i,alpha) for i in w]\n",
    "    fig, ax = plt.subplots(figsize=(8*fig_scale,4*fig_scale))\n",
    "    ax.set_xlabel('w')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.plot(w, rp, lw=1.5*fig_scale, ls=':', c='b', label='L2 with alpha={}'.format(alpha))\n",
    "    ax.plot(w, lp, lw=1.5*fig_scale, ls=':', c='r', label='L1 with alpha={}'.format(alpha))\n",
    "    ax.plot(w, f, lw=2*fig_scale, ls='-', c='k', label='Least Squares loss')\n",
    "    ax.plot(w, r, lw=2*fig_scale, ls='-', c='b', label='Loss + L2 (Ridge)'.format(alpha))\n",
    "    ax.plot(w, l, lw=2*fig_scale, ls='-', c='r', label='Loss + L1 (Lasso)'.format(alpha))\n",
    "    opt_f = np.argmin(f)\n",
    "    ax.scatter(w[opt_f], f[opt_f], c=\"k\", s=50*fig_scale)\n",
    "    opt_r = np.argmin(r)\n",
    "    ax.scatter(w[opt_r], r[opt_r], c=\"b\", s=50*fig_scale)\n",
    "    opt_l = np.argmin(l)\n",
    "    ax.scatter(w[opt_l], l[opt_l], c=\"r\", s=50*fig_scale)\n",
    "    ax.legend()\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.ylim(-0.1,1)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_losses(alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In 2D (for 2 model weights $w_1$ and $w_2$)\n",
    "    - The least squared loss is a 2D convex function in this space\n",
    "    - For illustration, assume that L1 loss = L2 loss = 1\n",
    "        - L1 loss ($\\Sigma |w_i|$): every {$w_1, w_2$} falls on the diamond\n",
    "        - L2 loss ($\\Sigma w_i^2$): every {$w_1, w_2$} falls on the circle\n",
    "    - For L1, the loss is minimized if $w_1$ or $w_2$ is 0 (rarely so for L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_loss_interpretation():\n",
    "    line = np.linspace(-1.5, 1.5, 1001)\n",
    "    xx, yy = np.meshgrid(line, line)\n",
    "\n",
    "    l2 = xx ** 2 + yy ** 2\n",
    "    l1 = np.abs(xx) + np.abs(yy)\n",
    "    rho = 0.7\n",
    "    elastic_net = rho * l1 + (1 - rho) * l2\n",
    "\n",
    "    plt.figure(figsize=(5*fig_scale, 4*fig_scale))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    elastic_net_contour = plt.contour(xx, yy, elastic_net, levels=[1], linewidths=2*fig_scale, colors=\"darkorange\")\n",
    "    l2_contour = plt.contour(xx, yy, l2, levels=[1], linewidths=2*fig_scale, colors=\"c\")\n",
    "    l1_contour = plt.contour(xx, yy, l1, levels=[1], linewidths=2*fig_scale, colors=\"navy\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.spines['left'].set_position('center')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['bottom'].set_position('center')\n",
    "    ax.spines['top'].set_color('none')\n",
    "\n",
    "    plt.clabel(elastic_net_contour, inline=1, fontsize=12*fig_scale,\n",
    "               fmt={1.0: 'elastic-net'}, manual=[(-0.6, -0.6)])\n",
    "    plt.clabel(l2_contour, inline=1, fontsize=12*fig_scale,\n",
    "               fmt={1.0: 'L2'}, manual=[(-0.5, -0.5)])\n",
    "    plt.clabel(l1_contour, inline=1, fontsize=12*fig_scale,\n",
    "               fmt={1.0: 'L1'}, manual=[(-0.5, -0.5)])\n",
    "\n",
    "    x1 = np.linspace(0.5, 1.5, 100)\n",
    "    x2 = np.linspace(-1.0, 1.5, 100)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    Y = np.sqrt(np.square(X1/2-0.7) + np.square(X2/4-0.28))\n",
    "    cp = plt.contour(X1, X2, Y)\n",
    "    #plt.clabel(cp, inline=1, fontsize=10)\n",
    "    ax.tick_params(axis='both', pad=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_loss_interpretation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Elastic-Net\n",
    "\n",
    "* Adds both L1 and L2 regularization:\n",
    "\n",
    "$$\\mathcal{L}_{Elastic} = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 + \\alpha \\rho \\sum_{i=0}^{p} |w_i| + \\alpha (1 -  \\rho) \\sum_{i=0}^{p} w_i^2$$ \n",
    "\n",
    "* $\\rho$ is the L1 ratio\n",
    "    * With $\\rho=1$, $\\mathcal{L}_{Elastic} = \\mathcal{L}_{Lasso}$\n",
    "    * With $\\rho=0$, $\\mathcal{L}_{Elastic} = \\mathcal{L}_{Ridge}$\n",
    "    * $0 < \\rho < 1$ sets a trade-off between L1 and L2.\n",
    "* Allows learning sparse models (like Lasso) while maintaining L2 regularization benefits\n",
    "    * E.g. if 2 features are correlated, Lasso likely picks one randomly, Elastic-Net keeps both \n",
    "* Weights can be optimized using coordinate descent (similar to Lasso) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other loss functions for regression\n",
    "* Huber loss: switches from squared loss to linear loss past a value $\\epsilon$\n",
    "    * More robust against outliers\n",
    "* Epsilon insensitive: ignores errors smaller than $\\epsilon$, and linear past that\n",
    "    * Aims to fit function so that residuals are at most $\\epsilon$\n",
    "    * Also known as Support Vector Regression (`SVR` in sklearn)\n",
    "* Squared Epsilon insensitive: ignores errors smaller than $\\epsilon$, and squared past that\n",
    "* These can all be solved with stochastic gradient descent\n",
    "    * `SGDRegressor` in sklearn\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/huber.png\" alt=\"ml\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear models for Classification\n",
    "Aims to find a hyperplane that separates the examples of each class.  \n",
    "For binary classification (2 classes), we aim to fit the following function: \n",
    "\n",
    "$\\hat{y} = w_1 * x_1 + w_2 * x_2 +... + w_p * x_p + w_0 > 0$  \n",
    "    \n",
    "When $\\hat{y}<0$, predict class -1, otherwise predict class +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "Xf, yf = mglearn.datasets.make_forge()\n",
    "fig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\n",
    "clf = LogisticRegression().fit(Xf, yf)\n",
    "mglearn.tools.plot_2d_separator(clf, Xf,\n",
    "                                ax=ax, alpha=.7, cm=mglearn.cm2)\n",
    "mglearn.discrete_scatter(Xf[:, 0], Xf[:, 1], yf, ax=ax, s=10*fig_scale)\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")\n",
    "ax.legend(['Class -1','Class 1']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* There are many algorithms for linear classification, differing in loss function, regularization techniques, and optimization method\n",
    "\n",
    "* Most common techniques:\n",
    "    * Convert target classes {neg,pos} to {0,1} and treat as a regression task\n",
    "        * Logistic regression (Log loss)\n",
    "        * Ridge Classification (Least Squares + L2 loss)\n",
    "    * Find hyperplane that maximizes the margin between classes\n",
    "        * Linear Support Vector Machines (Hinge loss)\n",
    "    * Neural networks without activation functions\n",
    "        * Perceptron (Perceptron loss)\n",
    "    * SGDClassifier: can act like any of these by choosing loss function\n",
    "        * Hinge, Log, Modified_huber, Squared_hinge, Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression\n",
    "* Aims to predict the _probability_ that a point belongs to the positive class\n",
    "* Converts target values {negative (blue), positive (red)} to {0,1}\n",
    "* Fits a _logistic_ (or _sigmoid_ or _S_ curve) function through these points\n",
    "    * Maps (-Inf,Inf) to a probability [0,1]\n",
    "    \n",
    "    $$ \\hat{y} = \\textrm{logistic}(f_{\\theta}(\\mathbf{x})) = \\frac{1}{1+e^{-f_{\\theta}(\\mathbf{x})}} $$\n",
    "    \n",
    "* E.g. in 1D: $ \\textrm{logistic}(x_1w_1+w_0) = \\frac{1}{1+e^{-x_1w_1-w_0}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def sigmoid(x,w1,w0):\n",
    "    return 1 / (1 + np.exp(-(x*w1+w0)))\n",
    "\n",
    "@interact\n",
    "def plot_logreg(w0=(-10.0,5.0,1),w1=(-1.0,3.0,0.3)):\n",
    "    fig, ax = plt.subplots(figsize=(8*fig_scale,3*fig_scale))\n",
    "    red = [Xf[i, 1] for i in range(len(yf)) if yf[i]==1]\n",
    "    blue = [Xf[i, 1] for i in range(len(yf)) if yf[i]==0]\n",
    "    ax.scatter(red, [1]*len(red), c='r', label='Positive class')\n",
    "    ax.scatter(blue, [0]*len(blue), c='b', label='Negative class')\n",
    "    x = np.linspace(min(-1, -w0/w1),max(6, -w0/w1))\n",
    "    ax.plot(x,sigmoid(x,w1,w0),lw=2*fig_scale,c='g', label='logistic(x*w1+w0)'.format(np.round(w0,2),np.round(w1,2)))\n",
    "    ax.axvline(x=(-w0/w1), ymin=0, ymax=1, label='Decision boundary')\n",
    "    ax.plot(x,x*w1+w0,lw=2*fig_scale,c='k',linestyle=':', label='y=x*w1+w0')\n",
    "    ax.set_xlabel(\"Feature\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_ylim(-0.05,1.05)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    # fitted solution\n",
    "    clf2 = LogisticRegression(C=100).fit(Xf[:, 1].reshape(-1, 1), yf)\n",
    "    w0 = clf2.intercept_\n",
    "    w1 = clf2.coef_[0][0]\n",
    "    plot_logreg(w0=w0,w1=w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Fitted solution to our 2D example:\n",
    "    * To get a binary prediction, choose a probability threshold (e.g. 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(C=100).fit(Xf, yf)\n",
    "\n",
    "def sigmoid2d(x1,x2,w0,w1,w2):\n",
    "    return 1 / (1 + np.exp(-(x2*w2+x1*w1+w0)))\n",
    "\n",
    "@interact\n",
    "def plot_logistic_fit(rotation=(0,360,10)):\n",
    "    w0 = lr_clf.intercept_\n",
    "    w1 = lr_clf.coef_[0][0]\n",
    "    w2 = lr_clf.coef_[0][1]\n",
    "\n",
    "    # plot surface of f\n",
    "    fig = plt.figure(figsize=(7*fig_scale,5*fig_scale))\n",
    "    ax = plt.axes(projection=\"3d\")\n",
    "    x0 = np.linspace(8, 16, 30)\n",
    "    x1 = np.linspace(-1, 6, 30)\n",
    "    X0, X1 = np.meshgrid(x0, x1)\n",
    "    \n",
    "    # Surface\n",
    "    ax.plot_surface(X0, X1, sigmoid2d(X0, X1, w0, w1, w2), rstride=1, cstride=1,\n",
    "                    cmap='bwr', edgecolor='none',alpha=0.5,label='sigmoid')\n",
    "    # Points\n",
    "    c=['b','r']\n",
    "    ax.scatter3D(Xf[:, 0], Xf[:, 1], yf, c=[c[i] for i in yf], s=10*fig_scale)\n",
    "    \n",
    "    # Decision boundary\n",
    "    # x2 = -(x1*w1 + w0)/w2\n",
    "    ax.plot3D(x0,-(x0*w1 + w0)/w2,[0.5]*len(x0), lw=1*fig_scale, c='k', linestyle=':')\n",
    "    z = np.linspace(0, 1, 31)\n",
    "    XZ, Z = np.meshgrid(x0, z)\n",
    "    YZ = -(XZ*w1 + w0)/w2    \n",
    "    ax.plot_wireframe(XZ, YZ, Z, rstride=5, lw=1*fig_scale, cstride=5, alpha=0.3, color='k',label='decision boundary')\n",
    "    ax.tick_params(axis='both', width=0, labelsize=10*fig_scale, pad=-4)\n",
    "\n",
    "    ax.set_xlabel('x0', labelpad=-6)\n",
    "    ax.set_ylabel('x1', labelpad=-6)\n",
    "    ax.get_zaxis().set_ticks([])\n",
    "    ax.view_init(30, rotation) # Use this to rotate the figure\n",
    "    plt.tight_layout()\n",
    "    #plt.legend() # Doesn't work yet, bug in matplotlib\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_logistic_fit(rotation=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Loss function: Cross-entropy\n",
    "* Models that return class probabilities can use _cross-entropy loss_\n",
    "    \n",
    "    $$\\mathcal{L_{log}}(\\mathbf{w}) = \\sum_{n=1}^{N} H(p_n,q_n) = - \\sum_{n=1}^{N} \\sum_{c=1}^{C} p_{n,c} log(q_{n,c}) $$\n",
    "    \n",
    "    * Also known as log loss, logistic loss, or maximum likelihood\n",
    "    * Based on true probabilities $p$ (0 or 1) and predicted probabilities $q$ over $N$ instances and $C$ classes\n",
    "        * Binary case (C=2): $\\mathcal{L_{log}}(\\mathbf{w}) = - \\sum_{n=1}^{N} \\big[ y_n log(\\hat{y}_n) + (1-y_n) log(1-\\hat{y}_n) \\big]$\n",
    "    * Penalty (or surprise) grows exponentially as difference between $p$ and $q$ increases\n",
    "    * Often used together with L2 (or L1) loss: $\\mathcal{L_{log}}'(\\mathbf{w}) = \\mathcal{L_{log}}(\\mathbf{w}) + \\alpha \\sum_{i} w_i^2 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def cross_entropy(yHat, y):\n",
    "    if y == 1:\n",
    "        return -np.log(yHat)\n",
    "    else:\n",
    "        return -np.log(1 - yHat)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6*fig_scale,2*fig_scale))\n",
    "x = np.linspace(0,1,100)\n",
    "\n",
    "ax.plot(x,cross_entropy(x, 1),lw=2*fig_scale,c='b',label='true label = 1', linestyle='-')\n",
    "ax.plot(x,cross_entropy(x, 0),lw=2*fig_scale,c='r',label='true label = 0', linestyle='-')\n",
    "ax.set_xlabel(r\"Predicted probability $\\hat{y}$\")\n",
    "ax.set_ylabel(\"Log loss\")\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Optimization methods (solvers) for cross-entropy loss\n",
    "* Gradient descent (only supports L2 regularization)\n",
    "    - Log loss is differentiable, so we can use (stochastic) gradient descent\n",
    "    - Variants thereof, e.g. Stochastic Average Gradient (SAG, SAGA)\n",
    "* Coordinate descent (supports both L1 and L2 regularization)\n",
    "    - Faster iteration, but may converge more slowly, has issues with saddlepoints\n",
    "    - Called `liblinear` in sklearn. Can't run in parallel.\n",
    "* Newton-Rhapson or Newton Conjugate Gradient (only L2):\n",
    "    - Uses the Hessian $H = \\big[\\frac{\\partial^2 \\mathcal{L}}{\\partial x_i \\partial x_j} \\big]$: $\\mathbf{w}^{s+1} = \\mathbf{w}^s-\\eta H^{-1}(\\mathbf{w}^s) \\nabla \\mathcal{L}(\\mathbf{w}^s)$\n",
    "    - Slow for large datasets. Works well if solution space is (near) convex\n",
    "* Quasi-Newton methods (only L2)\n",
    "    - Approximate, faster to compute\n",
    "    - E.g. Limited-memory Broyden–Fletcher–Goldfarb–Shanno (`lbfgs`)\n",
    "        - Default in sklearn for Logistic Regression\n",
    "* [Some hints on choosing solvers](https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451)\n",
    "    - Data scaling helps convergence, minimizes differences between solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### In practice\n",
    "* Logistic regression can also be found in `sklearn.linear_model`.\n",
    "    * `C` hyperparameter is the _inverse_ regularization strength: $C=\\alpha^{-1}$\n",
    "    * `penalty`: type of regularization: L1, L2 (default), Elastic-Net, or None\n",
    "    * `solver`: newton-cg, lbfgs (default), liblinear, sag, saga\n",
    "* Increasing C: less regularization, tries to overfit individual points\n",
    "\n",
    "``` python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1).fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "@interact\n",
    "def plot_lr(C_log=(-3,4,0.1)):\n",
    "    # Still using artificial data\n",
    "    fig, ax = plt.subplots(figsize=(6*fig_scale,3*fig_scale))\n",
    "    mglearn.discrete_scatter(Xf[:, 0], Xf[:, 1], yf, ax=ax, s=10*fig_scale)\n",
    "    lr = LogisticRegression(C=10**C_log).fit(Xf, yf)\n",
    "    w = lr.coef_[0]\n",
    "    xx = np.linspace(7, 13)\n",
    "    yy = (-w[0] * xx - lr.intercept_[0]) / w[1]\n",
    "    ax.plot(xx, yy, c='k')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(\"C = {:.3f}, w1={:.3f}, w2={:.3f}\".format(10**C_log,w[0],w[1]))\n",
    "    ax.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_lr(C_log=(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Analyze behavior on the breast cancer dataset\n",
    "    * Underfitting if C is too small, some overfitting if C is too large\n",
    "    * We use cross-validation because the dataset is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "spam_data = fetch_openml(name=\"banknote-authentication\", as_frame=True)\n",
    "X_C, y_C = spam_data.data, spam_data.target\n",
    "\n",
    "C=np.logspace(-3,2,num=20)\n",
    "test_score=[]\n",
    "train_score=[]\n",
    "for c in C:\n",
    "    lr = LogisticRegression(C=c)\n",
    "    scores = cross_validate(lr,X_C,y_C,cv=10, return_train_score=\"True\")\n",
    "    test_score.append(np.mean(scores['test_score']))\n",
    "    train_score.append(np.mean(scores['train_score']))\n",
    "fig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\n",
    "ax.set_xticks(range(20))\n",
    "ax.set_xticklabels(np.round(C,3))\n",
    "ax.set_xlabel('C')\n",
    "ax.plot(test_score, lw=2*fig_scale, label='test score')\n",
    "ax.plot(train_score, lw=2*fig_scale, label='train score')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Again, choose between L1 or L2 regularization (or elastic-net)\n",
    "* Small C overfits, L1 leads to sparse models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "X_C_train, X_C_test, y_C_train, y_C_test = train_test_split(X_C, y_C, random_state=0)\n",
    "\n",
    "@interact\n",
    "def plot_logreg(C=(0.01,100.0,0.1), penalty=['l1','l2']):\n",
    "    r = LogisticRegression(C=C, penalty=penalty, solver='liblinear').fit(X_C_train, y_C_train)\n",
    "    fig, ax = plt.subplots(figsize=(8*fig_scale,1.9*fig_scale))\n",
    "    ax.plot(r.coef_.T, 'o', markersize=6*fig_scale)\n",
    "    ax.set_title(\"C: {:.3f}, penalty: {}, score {:.2f} (training score {:.2f})\".format(C, penalty, r.score(X_C_test, y_C_test), r.score(X_C_train, y_C_train)),pad=0)\n",
    "    ax.set_xlabel(\"Coefficient index\", labelpad=0)\n",
    "    ax.set_ylabel(\"Coeff. magnitude\", labelpad=0, fontsize=10*fig_scale)\n",
    "    ax.tick_params(axis='both', pad=0)\n",
    "    ax.hlines(0, 30, len(r.coef_))\n",
    "    ax.set_ylim(-30, 30)\n",
    "    ax.set_xlim(0, 30);\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_logreg(0.001, 'l2')\n",
    "    plot_logreg(100, 'l2')\n",
    "    plot_logreg(100, 'l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge Classification\n",
    "* Instead of log loss, we can also use ridge loss:\n",
    "    \n",
    "    $$\\mathcal{L}_{Ridge} = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 + \\alpha \\sum_{i=0}^{p} w_i^2$$\n",
    "    \n",
    "* In this case, target values {negative, positive} are converted to {-1,1}\n",
    "* Can be solved similarly to Ridge regression:\n",
    "    * Closed form solution (a.k.a. Cholesky)\n",
    "    * Gradient descent and variants\n",
    "        * E.g. Conjugate Gradient (CG) or Stochastic Average Gradient (SAG,SAGA)\n",
    "    * Use Cholesky for smaller datasets, Gradient descent for larger ones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Support vector machines\n",
    "- Decision boundaries close to training points may generalize badly\n",
    "    - Very similar (nearby) test point are classified as the other class\n",
    "- Choose a boundary that is as far away from training points as possible\n",
    "- The __support vectors__ are the training samples closest to the hyperplane\n",
    "- The __margin__ is the distance between the separating hyperplane and the _support vectors_\n",
    "- Hence, our objective is to _maximize the margin_\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/05_margin.png\" alt=\"ml\" style=\"width: 1250px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solving SVMs with Lagrange Multipliers\n",
    "* Imagine a hyperplane (green) $y= \\sum_1^p \\mathbf{w}_i * \\mathbf{x}_i + w_0$ that has slope $\\mathbf{w}$, value '+1' for the positive (red) support vectors, and '-1' for the negative (blue) ones\n",
    "    * Margin between the boundary and support vectors is $\\frac{y-w_0}{||\\mathbf{w}||}$, with $||\\mathbf{w}|| = \\sum_i^p w_i^2$\n",
    "    * We want to find the weights that maximize $\\frac{1}{||\\mathbf{w}||}$. We can also do that by maximizing $\\frac{1}{||\\mathbf{w}||^2}$\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# we create 40 separable points\n",
    "np.random.seed(0)\n",
    "sX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\n",
    "sY = [0] * 20 + [1] * 20\n",
    "\n",
    "# fit the model\n",
    "s_clf = SVC(kernel='linear')\n",
    "s_clf.fit(sX, sY)\n",
    "\n",
    "@interact\n",
    "def plot_svc_fit(rotationX=(0,20,1),rotationY=(90,180,1)):\n",
    "    # get the separating hyperplane\n",
    "    w = s_clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(-5, 5)\n",
    "    yy = a * xx - (s_clf.intercept_[0]) / w[1]\n",
    "    zz = np.linspace(-2, 2, 30)\n",
    "\n",
    "    # plot the parallels to the separating hyperplane that pass through the\n",
    "    # support vectors\n",
    "    b = s_clf.support_vectors_[0]\n",
    "    yy_down = a * xx + (b[1] - a * b[0])\n",
    "    b = s_clf.support_vectors_[-1]\n",
    "    yy_up = a * xx + (b[1] - a * b[0])\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    fig = plt.figure(figsize=(7*fig_scale,4.5*fig_scale))\n",
    "    ax = plt.axes(projection=\"3d\")\n",
    "    ax.plot3D(xx, yy, [0]*len(xx), 'k-')\n",
    "    ax.plot3D(xx, yy_down, [0]*len(xx), 'k--')\n",
    "    ax.plot3D(xx, yy_up, [0]*len(xx), 'k--')\n",
    "\n",
    "    ax.scatter3D(s_clf.support_vectors_[:, 0], s_clf.support_vectors_[:, 1], [0]*len(s_clf.support_vectors_[:, 0]),\n",
    "                s=85*fig_scale, edgecolors='k', c='w')\n",
    "    ax.scatter3D(sX[:, 0], sX[:, 1], [0]*len(sX[:, 0]), c=sY, cmap=plt.cm.bwr, s=10*fig_scale )\n",
    "\n",
    "\n",
    "    # Planes\n",
    "    XX, YY = np.meshgrid(xx, yy)\n",
    "    if interactive:\n",
    "        ZZ = w[0]*XX+w[1]*YY+clf.intercept_[0]\n",
    "    else: # rescaling (for prints) messes up the Z values\n",
    "        ZZ = w[0]*XX/fig_scale+w[1]*YY/fig_scale+clf.intercept_[0]*fig_scale/2\n",
    "    ax.plot_wireframe(XX, YY, XX*0, rstride=5, cstride=5, alpha=0.3, color='k', label='XY plane')\n",
    "    ax.plot_wireframe(XX, YY, ZZ, rstride=5, cstride=5, alpha=0.3, color='g', label='hyperplane')\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    ax.view_init(rotationX, rotationY) # Use this to rotate the figure\n",
    "    ax.dist = 6\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "if not interactive:\n",
    "    plot_svc_fit(9,135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Geometric interpretation\n",
    "- We want to maximize $f = \\frac{1}{||w||^2}$ (blue contours)\n",
    "- The hyperplane (red) must be $> 1$ for all positive examples:  \n",
    "$g(\\mathbf{w}) = \\mathbf{w} \\mathbf{x_i} + w_0 > 1 \\,\\,\\, \\forall{i}, y(i)=1$\n",
    "- Find the weights $\\mathbf{w}$ that satify $g$ but maximize $f$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/LagrangeMultipliers3D.png\" alt=\"ml\" style=\"width: 950px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Solution\n",
    "* A quadratic loss function with linear constraints can be solved with *Lagrangian multipliers*\n",
    "* This works by assigning a weight $a_i$ (called a dual coefficient) to every data point $x_i$\n",
    "    * They reflect how much individual points influence the weights $\\mathbf{w}$\n",
    "    * The points with non-zero $a_i$ are the _support vectors_\n",
    "* Next, solve the following **Primal** objective:\n",
    "    * $y_i=\\pm1$ is the correct class for example $x_i$\n",
    "\n",
    "$$\\mathcal{L}_{Primal} = \\frac{1}{2} ||\\mathbf{w}||^2 - \\sum_{i=1}^{n} a_i y_i (\\mathbf{w} \\mathbf{x_i}  + w_0) + \\sum_{i=1}^{n} a_i $$\n",
    "\n",
    "so that\n",
    "\n",
    "$$ \\mathbf{w} = \\sum_{i=1}^{n} a_i y_i \\mathbf{x_i} $$\n",
    "$$ a_i \\geq 0 \\quad \\text{and} \\quad \\sum_{i=1}^{l} a_i y_i = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* It has a **Dual** formulation as well (See 'Elements of Statistical Learning' for the derivation):\n",
    "\n",
    "$$\\mathcal{L}_{Dual} = \\sum_{i=1}^{l} a_i - \\frac{1}{2} \\sum_{i,j=1}^{l} a_i a_j y_i y_j (\\mathbf{x_i} \\mathbf{x_j}) $$\n",
    "\n",
    "so that\n",
    "\n",
    "$$ a_i \\geq 0 \\quad \\text{and} \\quad \\sum_{i=1}^{l} a_i y_i = 0 $$\n",
    "\n",
    "* Computes the dual coefficients directly. A number $l$ of these are non-zero (sparseness).\n",
    "    * Dot product $\\mathbf{x_i} \\mathbf{x_j}$ can be interpreted as the closeness between points $\\mathbf{x_i}$ and $\\mathbf{x_j}$\n",
    "    * $\\mathcal{L}_{Dual}$ increases if nearby support vectors $\\mathbf{x_i}$ with high weights $a_i$ have different class $y_i$\n",
    "    * $\\mathcal{L}_{Dual}$ also increases with the number of support vectors $l$ and their weights $a_i$\n",
    "\n",
    "* Can be solved with quadratic programming, e.g. Sequential Minimal Optimization (SMO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example result. The circled samples are support vectors, together with their coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Plot SVM support vectors\n",
    "def plot_linear_svm(X,y,C,ax):\n",
    "\n",
    "    clf = SVC(kernel='linear', C=C)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # get the separating hyperplane\n",
    "    w = clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(-5, 5)\n",
    "    yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "\n",
    "    # plot the parallels to the separating hyperplane\n",
    "    yy_down = (-1-w[0]*xx-clf.intercept_[0])/w[1]\n",
    "    yy_up = (1-w[0]*xx-clf.intercept_[0])/w[1]\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    ax.set_title('C = %s' % C)\n",
    "    ax.plot(xx, yy, 'k-')\n",
    "    ax.plot(xx, yy_down, 'k--')\n",
    "    ax.plot(xx, yy_up, 'k--')\n",
    "    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "                s=85*fig_scale, edgecolors='gray', c='w', zorder=10, lw=1*fig_scale)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.bwr)\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # Add coefficients\n",
    "    for i, coef in enumerate(clf.dual_coef_[0]):\n",
    "        ax.annotate(\"%0.2f\" % (coef), (clf.support_vectors_[i, 0]+0.1,clf.support_vectors_[i, 1]+0.35), fontsize=10*fig_scale, zorder=11)\n",
    "\n",
    "    ax.set_xlim(np.min(X[:, 0])-0.5, np.max(X[:, 0])+0.5)\n",
    "    ax.set_ylim(np.min(X[:, 1])-0.5, np.max(X[:, 1])+0.5)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "\n",
    "# we create 40 separable points\n",
    "np.random.seed(0)\n",
    "svm_X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\n",
    "svm_Y = [0] * 20 + [1] * 20\n",
    "svm_fig, svm_ax = plt.subplots(figsize=(8*fig_scale,5*fig_scale))\n",
    "plot_linear_svm(svm_X,svm_Y,1,svm_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Making predictions\n",
    "- $a_i$ will be *0* if the training point lies on the right side of the decision boundary and outside the margin \n",
    "- The training samples for which $a_i$ is not 0 are the _support vectors_ \n",
    "- Hence, the SVM model is completely defined by the support vectors and their dual coefficients (weights)\n",
    "\n",
    "- Knowing the dual coefficients $a_i$, we can find the weights $w$ for the maximal margin separating hyperplane:  \n",
    "\n",
    "$$ \\mathbf{w} = \\sum_{i=1}^{l} a_i y_i \\mathbf{x_i} $$\n",
    "\n",
    "- Hence, we can classify a new sample $\\mathbf{u}$ by looking at the sign of $\\mathbf{w}\\mathbf{u}+w_0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### SVMs and kNN\n",
    "* Remember, we will classify a new point $\\mathbf{u}$ by looking at the sign of:  \n",
    "\n",
    "$$f(x) = \\mathbf{w}\\mathbf{u}+w_0 = \\sum_{i=1}^{l} a_i y_i \\mathbf{x_i}\\mathbf{u}+w_0$$\n",
    "\n",
    "* _Weighted k-nearest neighbor_ is a generalization of the k-nearest neighbor classifier. It classifies points by evaluating:  \n",
    "\n",
    "$$f(x) = \\sum_{i=1}^{k} a_i y_i dist(x_i, u)^{-1}$$\n",
    "\n",
    "* Hence: SVM's predict much the same way as k-NN, only:\n",
    "    - They only consider the truly important points (the support vectors): _much_ faster\n",
    "        - The number of neighbors is the number of support vectors\n",
    "    - The distance function is an _inner product of the inputs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Regularized (soft margin) SVMs\n",
    "\n",
    "- If the data is not linearly separable, (hard) margin maximization becomes meaningless\n",
    "- Relax the contraint by allowing an error $\\xi_{i}$: $y_i (\\mathbf{w}\\mathbf{x_i} + w_0) \\geq 1 - \\xi_{i}$\n",
    "- Or (since $\\xi_{i} \\geq 0$):\n",
    "\n",
    "$$\\xi_{i} =  max(0,1-y_i\\cdot(\\mathbf{w}\\mathbf{x_i} + w_0))$$\n",
    "\n",
    "- The sum over all points is called _hinge loss_: $\\sum_i^n \\xi_{i}$\n",
    "- Attenuating the error component with a hyperparameter $C$, we get the objective\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w}) = ||\\mathbf{w}||^2 + C \\sum_i^n \\xi_{i}$$\n",
    "\n",
    "- Can still be solved with quadratic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def hinge_loss(yHat, y):\n",
    "    if y == 1:\n",
    "        return np.maximum(0,1-yHat)\n",
    "    else:\n",
    "        return np.maximum(0,1+yHat)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6*fig_scale,2*fig_scale))\n",
    "x = np.linspace(-2,2,100)\n",
    "\n",
    "ax.plot(x,hinge_loss(x, 1),lw=2*fig_scale,c='b',label='true label = 1', linestyle='-')\n",
    "ax.plot(x,hinge_loss(x, 0),lw=2*fig_scale,c='r',label='true label = 0', linestyle='-')\n",
    "ax.set_xlabel(r\"Prediction value $\\hat{y}$\")\n",
    "ax.set_ylabel(\"Hinge loss\")\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Least Squares SVMs\n",
    "\n",
    "- We can also use the _squares_ of all the errors, or squared hinge loss: $\\sum_i^n \\xi_{i}^2$\n",
    "- This yields the Least Squares SVM objective\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w}) = ||\\mathbf{w}||^2 + C \\sum_i^n \\xi_{i}^2$$\n",
    "\n",
    "- Can be solved with Lagrangian Multipliers and a set of linear equations\n",
    "    - Still yields support vectors and still allows kernelization\n",
    "    - Support vectors are not sparse, but pruning techniques exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6*fig_scale,2*fig_scale))\n",
    "x = np.linspace(-2,2,100)\n",
    "\n",
    "ax.plot(x,hinge_loss(x, 1)** 2,lw=2*fig_scale,c='b',label='true label = 1', linestyle='-')\n",
    "ax.plot(x,hinge_loss(x, 0)** 2,lw=2*fig_scale,c='r',label='true label = 0', linestyle='-')\n",
    "ax.set_xlabel(r\"Prediction value $\\hat{y}$\")\n",
    "ax.set_ylabel(\"Squared hinge loss\")\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Effect of regularization on margin and support vectors\n",
    "- SVM's Hinge loss acts like L1 regularization, yields sparse models\n",
    "- C is the _inverse_ regularization strength (inverse of $\\alpha$ in Lasso)\n",
    "    - Larger C: fewer support vectors, smaller margin, more overfitting\n",
    "    - Smaller C: more support vectors, wider margin, less overfitting\n",
    "- Needs to be tuned carefully to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, svm_axes = plt.subplots(nrows=1, ncols=2, figsize=(12*fig_scale, 4*fig_scale))\n",
    "plot_linear_svm(svm_X,svm_Y,1,svm_axes[0])\n",
    "plot_linear_svm(svm_X,svm_Y,0.05,svm_axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Same for non-linearly separable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "svm_X = np.r_[np.random.randn(20, 2) - [1, 1], np.random.randn(20, 2) + [1, 1]]\n",
    "fig, svm_axes = plt.subplots(nrows=1, ncols=2, figsize=(12*fig_scale, 5*fig_scale))\n",
    "plot_linear_svm(svm_X,svm_Y,1,svm_axes[0])\n",
    "plot_linear_svm(svm_X,svm_Y,0.05,svm_axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Large C values can lead to overfitting (e.g. fitting noise), small values can lead to underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_linear_svc_regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### SVMs in scikit-learn\n",
    "\n",
    "- `svm.LinearSVC`: faster for large datasets\n",
    "    - Allows choosing between the primal or dual. Primal recommended when $n$ >> $p$\n",
    "    - Returns `coef_` ($\\mathbf{w}$) and `intercept_` ($w_0$)\n",
    "- `svm.SVC` with `kernel=linear`: allows _kernelization_ (see later)\n",
    "    - Also returns `support_vectors_` (the support vectors) and the `dual_coef_` $a_i$\n",
    "    - Scales at least quadratically with the number of samples $n$\n",
    "- `svm.LinearSVR` and `svm.SVR` are variants for regression\n",
    "``` python\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, Y)\n",
    "print(\"Support vectors:\", clf.support_vectors_[:])\n",
    "print(\"Coefficients:\", clf.dual_coef_[:])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Linearly separable dat\n",
    "np.random.seed(0)\n",
    "X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\n",
    "Y = [0] * 20 + [1] * 20\n",
    "\n",
    "# Fit the model\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# Get the support vectors and weights\n",
    "print(\"Support vectors:\")\n",
    "print(clf.support_vectors_[:])\n",
    "print(\"Coefficients:\")\n",
    "print(clf.dual_coef_[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solving SVMs with Gradient Descent\n",
    "* Soft-margin SVMs can, alternatively, be solved using gradient decent\n",
    "    * Good for large datasets, but does not yield support vectors or kernelization\n",
    "* Squared Hinge is differentiable\n",
    "* Hinge is not differentiable but convex, and has a subgradient:\n",
    "\n",
    "$$\\mathcal{L_{Hinge}}(\\mathbf{w}) =  max(0,1-y_i (\\mathbf{w}\\mathbf{x_i} + w_0))$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L_{Hinge}}}{\\partial w_i} =  \\begin{cases}-y_i x_i & y_i (\\mathbf{w}\\mathbf{x_i} + w_0) < 1\\\\ 0 & \\text{otherwise} \\\\ \\end{cases}$$\n",
    "\n",
    "* Can be solved with (stochastic) gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6*fig_scale,2*fig_scale))\n",
    "x = np.linspace(-2,2,100)\n",
    "\n",
    "ax.plot(x,hinge_loss(x, 1),lw=2*fig_scale,c='b',label='true label = 1', linestyle='-')\n",
    "ax.plot(x,hinge_loss(x, 0),lw=2*fig_scale,c='r',label='true label = 0', linestyle='-')\n",
    "ax.set_xlabel(r\"Prediction value $\\hat{y}$\")\n",
    "ax.set_ylabel(\"Hinge loss\")\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Generalized SVMs\n",
    "* Because the derivative of hinge loss is undefined at y=1, smoothed versions are often used:\n",
    "    * Squared hinge loss: yields _least squares SVM_\n",
    "        - Equivalent to Ridge classification (with different solver)\n",
    "    * Modified Huber loss: squared hinge, but linear after -1. Robust against outliers\n",
    "* Log loss can also be used (equivalent to logistic regression) \n",
    "* In sklearn, `SGDClassifier` can be used with any of these. Good for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def modified_huber_loss(y_true, y_pred):\n",
    "    z = y_pred * y_true\n",
    "    loss = -4 * z\n",
    "    loss[z >= -1] = (1 - z[z >= -1]) ** 2\n",
    "    loss[z >= 1.] = 0\n",
    "    return loss\n",
    "\n",
    "xmin, xmax = -4, 4\n",
    "xx = np.linspace(xmin, xmax, 100)\n",
    "lw = 2*fig_scale\n",
    "fig, ax = plt.subplots(figsize=(8*fig_scale,4*fig_scale))\n",
    "plt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], 'k-', lw=lw,\n",
    "         label=\"Zero-one loss\")\n",
    "plt.plot(xx, np.where(xx < 1, 1 - xx, 0), 'b-', lw=lw,\n",
    "         label=\"Hinge loss\")\n",
    "plt.plot(xx, -np.minimum(xx, 0), color='yellowgreen', lw=lw,\n",
    "         label=\"Perceptron loss\")\n",
    "plt.plot(xx, np.log2(1 + np.exp(-xx)), 'r-', lw=lw,\n",
    "         label=\"Log loss\")\n",
    "plt.plot(xx, np.where(xx < 1, 1 - xx, 0) ** 2, 'c-', lw=lw,\n",
    "         label=\"Squared hinge loss\")\n",
    "plt.plot(xx, modified_huber_loss(xx, 1), color='darkorchid', lw=lw,\n",
    "         linestyle='--', label=\"Modified Huber loss\")\n",
    "plt.ylim((0, 7))\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(r\"Decision function $f(x)$\")\n",
    "plt.ylabel(\"$Loss(y=1, f(x))$\")\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perceptron\n",
    "* Represents a single neuron (node) with inputs $x_i$, a bias $w_0$, and output $y$\n",
    "* Each connection has a (synaptic) weight $w_i$. The node outputs $\\hat{y} = \\sum_{i}^n x_{i}w_i + w_0$\n",
    "* The _activation function_ predicts 1 if $\\mathbf{xw} + w_0 > 0$, -1 otherwise\n",
    "* Weights can be learned with (stochastic) gradient descent and Hinge(0) loss\n",
    "    * Updated _only_ on misclassification, corrects output by $\\pm1$\n",
    "    \n",
    "    $$\\mathcal{L}_{Perceptron} = max(0,-y_i (\\mathbf{w}\\mathbf{x_i} + w_0))$$\n",
    "    \n",
    "    $$\\frac{\\partial \\mathcal{L_{Perceptron}}}{\\partial w_i} =  \\begin{cases}-y_i x_i & y_i (\\mathbf{w}\\mathbf{x_i} + w_0) < 0\\\\ 0 & \\text{otherwise} \\\\ \\end{cases}$$\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/perceptron.png\" alt=\"ml\" style=\"margin: 0 auto; width: 1500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Models for multiclass classification\n",
    "### one-vs-rest (aka one-vs-all)\n",
    "\n",
    "* Learn a binary model for each class vs. all other classes\n",
    "* Create as many binary models as there are classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(random_state=42)\n",
    "linear_svm = LinearSVC().fit(X, y)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7*fig_scale,5*fig_scale)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y, s=10*fig_scale)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
    "                                  mglearn.cm3.colors):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color, lw=2*fig_scale)\n",
    "plt.ylim(-10, 15)\n",
    "plt.xlim(-10, 8)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
    "            'Line class 2'], loc=(1.01, 0.3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Every binary classifiers makes a prediction, the one with the highest score (>0) wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=0.3)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y, s=10*fig_scale)\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n",
    "                                  mglearn.cm3.colors):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color, lw=2*fig_scale)\n",
    "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
    "            'Line class 2'], loc=(1.01, 0.3))\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### one-vs-one\n",
    "* An alternative is to learn a binary model for every _combination_ of two classes\n",
    "    * For $C$ classes, this results in $\\frac{C(C-1)}{2}$ binary models\n",
    "    * Each point is classified according to a majority vote amongst all models\n",
    "    * Can also be a 'soft vote': sum up the probabilities (or decision values) for all models. The class with the highest sum wins.\n",
    "* Requires more models than one-vs-rest, but training each one is faster\n",
    "    * Only the examples of 2 classes are included in the training data\n",
    "* Recommended for algorithms than learn well on small datasets \n",
    "    * Especially SVMs and Gaussian Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "td {font-size: 16px}\n",
    "th {font-size: 16px}\n",
    ".rendered_html table, .rendered_html td, .rendered_html th {\n",
    "    font-size: 16px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear models overview\n",
    "\n",
    "| Name | Representation | Loss function | Optimization | Regularization |\n",
    "|---|---|---|---|---|\n",
    "| Least squares    | Linear function (R) | SSE | CFS or SGD | None |\n",
    "| Ridge | Linear function (R) | SSE + L2 | CFS or SGD | L2 strength ($\\alpha$)  |\n",
    "| Lasso | Linear function (R) | SSE + L1 | Coordinate descent   | L1 strength ($\\alpha$)  |\n",
    "| Elastic-Net | Linear function (R) | SSE + L1 + L2 | Coordinate descent   | $\\alpha$, L1 ratio ($\\rho$)  |\n",
    "| SGDRegressor | Linear function (R) | SSE, Huber, $\\epsilon$-ins,... + L1/L2 | SGD   | L1/L2, $\\alpha$  |\n",
    "| Logistic regression | Linear function (C) | Log + L1/L2 | SGD, coordinate descent,...   | L1/L2, $\\alpha$ |\n",
    "| Ridge classification | Linear function (C) | SSE + L2 | CFS or SGD   | L2 strength ($\\alpha$) |\n",
    "| Linear SVM | Support Vectors | Hinge(1) | Quadratic programming or SGD | Cost (C) |\n",
    "| Least Squares SVM | Support Vectors | Squared Hinge | Linear equations or SGD | Cost (C) |\n",
    "| Perceptron | Linear function (C) | Hinge(0) | SGD | None |\n",
    "| SGDClassifier | Linear function (C) | Log, (Sq.) Hinge, Mod. Huber,... | SGD | L1/L2, $\\alpha$ |\n",
    "\n",
    "* SSE: Sum of Squared Errors\n",
    "* CFS: Closed-form solution\n",
    "* SGD: (Stochastic) Gradient Descent and variants\n",
    "* (R)egression, (C)lassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Linear models\n",
    "    - Good for very large datasets (scalable)\n",
    "    - Good for very high-dimensional data (not for low-dimensional data)\n",
    "- Can be used to fit non-linear or low-dim patterns as well (see later)\n",
    "    - Preprocessing: e.g. Polynomial or Poisson transformations\n",
    "    - Generalized linear models (kernelization)\n",
    "- Regularization is important. Tune the regularization strength ($\\alpha$)\n",
    "    - Ridge (L2): Good fit, sometimes sensitive to outliers\n",
    "    - Lasso (L1): Sparse models: fewer features, more interpretable, faster\n",
    "    - Elastic-Net: Trade-off between both, e.g. for correlated features \n",
    "- Most can be solved by different optimizers (solvers)\n",
    "    - Closed form solutions or quadratic/linear solvers for smaller datasets\n",
    "    - Gradient descent variants (SGD,CD,SAG,CG,...) for larger ones\n",
    "- Multi-class classification can be done using a one-vs-all approach"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_metadata": {
   "author": "Joaquin Vanschoren",
   "title": "Introduction"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
